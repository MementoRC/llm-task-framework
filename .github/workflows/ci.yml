name: CI

on:
  push:
    branches: [ main, master, development ]
  pull_request:
    branches: [ main, master, development ]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHONPATH: src
  PYTHONUNBUFFERED: "1"
  FORCE_COLOR: "1"

jobs:
  # Smart change detection for conditional job execution
  detect-changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    timeout-minutes: 2
    outputs:
      has-python-changes: ${{ steps.changes.outputs.python }}
      has-docs-changes: ${{ steps.changes.outputs.docs }}
      has-security-changes: ${{ steps.changes.outputs.security }}
      has-performance-changes: ${{ steps.changes.outputs.performance }}
      has-ci-changes: ${{ steps.changes.outputs.ci }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Detect file changes
      uses: dorny/paths-filter@v2
      id: changes
      with:
        filters: |
          python:
            - 'src/**'
            - 'tests/**'
            - 'pyproject.toml'
            - 'pixi.lock'
          docs:
            - 'docs/**'
            - 'mkdocs.yml'
            - '*.md'
          security:
            - '.secrets.baseline'
            - 'security/**'
            - '.github/workflows/security.yml'
          performance:
            - 'tests/**/test_*performance*'
            - 'tests/**/test_*benchmark*'
            - 'locustfile.py'
          ci:
            - '.github/workflows/**'
            - 'pyproject.toml'
            - 'pixi.lock'

    - name: Change detection summary
      run: |
        echo "## 🔍 Change Detection Results" >> $GITHUB_STEP_SUMMARY
        echo "| Component | Changed | Action |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|---------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Python Code | ${{ steps.changes.outputs.python }} | ${{ steps.changes.outputs.python == 'true' && 'Run full test suite' || 'Skip extended testing' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Documentation | ${{ steps.changes.outputs.docs }} | ${{ steps.changes.outputs.docs == 'true' && 'Build docs' || 'Skip docs build' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Security | ${{ steps.changes.outputs.security }} | ${{ steps.changes.outputs.security == 'true' && 'Run security scans' || 'Basic security only' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Performance | ${{ steps.changes.outputs.performance }} | ${{ steps.changes.outputs.performance == 'true' && 'Run performance tests' || 'Skip performance tests' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| CI/CD | ${{ steps.changes.outputs.ci }} | ${{ steps.changes.outputs.ci == 'true' && 'Full validation' || 'Standard validation' }} |" >> $GITHUB_STEP_SUMMARY

  # Quick feedback loop - fast checks for immediate developer feedback
  quick-checks:
    name: Quick Validation (Fast Feedback)
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    - name: Cache Python packages
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.pixi/cache
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml', '**/pixi.lock') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Cache pre-commit hooks
      uses: actions/cache@v4
      with:
        path: ~/.cache/pre-commit
        key: ${{ runner.os }}-pre-commit-${{ hashFiles('.pre-commit-config.yaml') }}
        restore-keys: |
          ${{ runner.os }}-pre-commit-

    - name: Fast linting check
      run: pixi run -e dev lint || true

    - name: Fast unit tests (no coverage)
      run: pixi run -e dev test-fast -m "not integration and not mcp" || true

    - name: Fast package import test
      run: |
        pixi run -e dev python -c "import llm_task_framework; print('✅ Package imports successfully')" || echo "❌ Package import failed"

    - name: Quick feedback summary
      if: always()
      run: |
        echo "## ⚡ Quick Feedback Results" >> $GITHUB_STEP_SUMMARY
        echo "Fast validation completed in under 5 minutes for immediate developer feedback." >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Note:** Full CI pipeline continues with comprehensive testing, quality checks, and security scanning." >> $GITHUB_STEP_SUMMARY

  quality-checks:
    name: Quality Checks
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [detect-changes, quick-checks]
    if: needs.detect-changes.outputs.has-python-changes == 'true' || needs.detect-changes.outputs.has-ci-changes == 'true'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    - name: Cache Python packages
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.pixi/cache
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml', '**/pixi.lock') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Cache pre-commit hooks
      uses: actions/cache@v4
      with:
        path: ~/.cache/pre-commit
        key: ${{ runner.os }}-pre-commit-${{ hashFiles('.pre-commit-config.yaml') }}
        restore-keys: |
          ${{ runner.os }}-pre-commit-

    - name: Cache security scan databases
      uses: actions/cache@v4
      with:
        path: |
          ~/.local/share/bandit
          ~/.cache/safety
          ~/.cache/pip-audit
        key: ${{ runner.os }}-security-db-${{ steps.date.outputs.week }}
        restore-keys: |
          ${{ runner.os }}-security-db-

    - name: Set cache date
      id: date
      run: echo "week=$(date +'%Y-W%V')" >> $GITHUB_OUTPUT

    - name: Check code formatting
      run: pixi run -e dev format-check

    - name: Run linting
      run: pixi run -e dev lint

    - name: Run type checking
      run: pixi run -e dev typecheck

    - name: Run enhanced security scans
      run: pixi run -e security security-ci

    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports-${{ github.run_number }}
        path: |
          reports/bandit-report.json
          reports/safety-report.json
          reports/pip-audit-report.json
        retention-days: 30

    - name: Security scan summary
      if: always()
      run: |
        echo "## 🛡️ Security Scan Results" >> $GITHUB_STEP_SUMMARY
        echo "Security scans completed. Check artifacts for detailed reports." >> $GITHUB_STEP_SUMMARY
        if [ -f "reports/bandit-report.json" ]; then
          echo "- ✅ Static analysis (Bandit): Report generated" >> $GITHUB_STEP_SUMMARY
        fi
        if [ -f "reports/safety-report.json" ]; then
          echo "- ✅ Dependency scan (Safety): Report generated" >> $GITHUB_STEP_SUMMARY
        fi
        if [ -f "reports/pip-audit-report.json" ]; then
          echo "- ✅ Package audit (pip-audit): Report generated" >> $GITHUB_STEP_SUMMARY
        fi

  test:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }} [${{ matrix.test-category }}]
    runs-on: ${{ matrix.os }}
    timeout-minutes: 20
    needs: [detect-changes]
    if: needs.detect-changes.outputs.has-python-changes == 'true' || needs.detect-changes.outputs.has-ci-changes == 'true' || github.event_name == 'workflow_dispatch'
    strategy:
      fail-fast: false
      matrix:
        include:
          # Ubuntu: Full coverage (all markers except slow/benchmark/mcp/integration for unit, then integration separately)
          - os: ubuntu-latest
            python-version: "3.10"
            test-category: "full"
            pytest-marker: "not slow and not benchmark and not mcp"
          - os: ubuntu-latest
            python-version: "3.11"
            test-category: "full"
            pytest-marker: "not slow and not benchmark and not mcp"
          - os: ubuntu-latest
            python-version: "3.12"
            test-category: "full"
            pytest-marker: "not slow and not benchmark and not mcp"

          # macOS: Core tests (no slow, no network, no integration, no mcp, no benchmark)
          - os: macos-latest
            python-version: "3.12"
            test-category: "core"
            pytest-marker: "not slow and not network and not integration and not mcp and not benchmark"

          # Windows: Essential tests (no slow, no network, no integration, no mcp, no benchmark, no e2e, no load)
          - os: windows-latest
            python-version: "3.10"
            test-category: "essential"
            pytest-marker: "not slow and not network and not integration and not mcp and not benchmark and not e2e and not load"
          - os: windows-latest
            python-version: "3.12"
            test-category: "essential"
            pytest-marker: "not slow and not network and not integration and not mcp and not benchmark and not e2e and not load"

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    - name: Cache Python packages (matrix-specific)
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.pixi/cache
        key: ${{ runner.os }}-${{ matrix.python-version }}-pip-${{ hashFiles('**/pyproject.toml', '**/pixi.lock') }}
        restore-keys: |
          ${{ runner.os }}-${{ matrix.python-version }}-pip-
          ${{ runner.os }}-pip-

    - name: Cache test fixtures and pytest cache
      uses: actions/cache@v4
      with:
        path: |
          .pytest_cache
          tests/fixtures/data
        key: ${{ runner.os }}-${{ matrix.python-version }}-test-cache-${{ hashFiles('tests/**/*.py') }}
        restore-keys: |
          ${{ runner.os }}-${{ matrix.python-version }}-test-cache-
          ${{ runner.os }}-test-cache-

    # Removed explicit editable install step; Pixi handles editable install automatically via environment setup.

    - name: Run optimized tests with coverage
      shell: bash
      run: |
        echo "Running ${{ matrix.test-category }} tests for ${{ matrix.os }} Python ${{ matrix.python-version }}..."
        pixi run -e dev pytest tests -m "${{ matrix.pytest-marker }}" --cov=llm_task_framework --cov-report=xml:coverage.xml --cov-report=term
      env:
        # Disable LLM tests in CI unless API keys provided
        SKIP_LLM_TESTS: ${{ !secrets.ANTHROPIC_API_KEY && !secrets.OPENAI_API_KEY }}

    - name: Run basic integration tests (cross-platform, Ubuntu/macOS only)
      shell: bash
      run: |
        if [[ "${{ matrix.os }}" == "ubuntu-latest" ]] || [[ "${{ matrix.os }}" == "macos-latest" ]]; then
          echo "Running cross-platform integration tests..."
          pixi run -e dev pytest tests -m "integration and not mcp and not benchmark" --cov=llm_task_framework --cov-append || [ $? -eq 5 ]
        else
          echo "Skipping integration tests on ${{ matrix.os }} for performance optimization"
        fi
      env:
        SKIP_LLM_TESTS: ${{ !secrets.ANTHROPIC_API_KEY && !secrets.OPENAI_API_KEY }}

    - name: Upload coverage to Codecov
      if: matrix.python-version == '3.11' && matrix.os == 'ubuntu-latest'
      uses: codecov/codecov-action@v3
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  integration-tests:
    name: Advanced Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: [test]
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false

    # Service containers for enhanced integration testing (Ubuntu only)
    services:
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

      websocket-echo:
        image: mendhak/http-https-echo:31
        # This image supports WebSocket echo functionality via /ws endpoint
        # WebSocket endpoint available at ws://localhost:8080/ws
        ports:
          - 8080:8080
        env:
          HTTP_PORT: 8080
          HTTPS_PORT: 8443

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    # Pixi handles editable install automatically via environment setup.

    - name: Wait for Redis service
      run: |
        echo "Waiting for Redis service to be ready..."
        # Install redis-tools for health checks
        sudo apt-get update && sudo apt-get install -y redis-tools
        # Wait for Redis with timeout, fail if not ready
        timeout 30 bash -c 'until redis-cli -h localhost -p 6379 ping | grep PONG; do sleep 1; done'
        echo "✅ Redis service is ready."

    - name: Wait for WebSocket echo service
      run: |
        echo "Waiting for WebSocket echo service to be ready..."
        # Wait for the TCP port to be open (WebSocket server ready)
        for i in {1..30}; do
          if nc -z localhost 8080; then
            echo "✅ WebSocket echo service is ready."
            exit 0
          fi
          sleep 1
        done
        echo "❌ WebSocket echo service did not become ready in time."
        exit 1

    - name: Run advanced integration tests
      run: |
        echo "Running advanced integration tests with external services..."
        # Run advanced integration tests that may require external services
        # These run separately to avoid pollution of cross-platform tests
        pixi run -e dev pytest tests -m "integration and not benchmark" --cov=llm_task_framework --cov-report=xml:integration-coverage.xml --cov-fail-under=0 || [ $? -eq 5 ]
      env:
        # LLM API keys
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        # Service container connection details
        REDIS_URL: redis://localhost:6379
        WEBSOCKET_URL: ws://localhost:8080/ws
        TEST_WITH_SERVICES: true

    - name: Run MCP server tests (isolated)
      run: |
        echo "Running MCP server tests in isolation..."
        # Run MCP tests separately to prevent any server state pollution
        pixi run -e dev pytest tests -m "mcp and not benchmark" --cov=llm_task_framework --cov-append --cov-fail-under=0 || [ $? -eq 5 ]
      env:
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

    - name: Upload integration coverage to Codecov
      uses: codecov/codecov-action@v3
      if: always()
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        file: ./integration-coverage.xml
        flags: integration
        name: codecov-integration
        fail_ci_if_error: false

  performance-tests:
    name: Performance Testing
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [detect-changes, test]
    if: needs.detect-changes.outputs.has-performance-changes == 'true' || needs.detect-changes.outputs.has-python-changes == 'true' || github.event_name == 'workflow_dispatch'
    continue-on-error: true  # Don't fail CI if performance tests have issues

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    - name: Run benchmark tests
      run: |
        echo "Running performance benchmark tests..."
        pixi run -e performance perf-test || echo "Performance tests completed with warnings"

    - name: Generate performance profile
      run: |
        echo "Generating performance profile..."
        pixi run -e performance perf-profile || echo "Performance profiling completed"

    - name: Upload performance reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-reports-${{ github.run_number }}
        path: |
          reports/profile.stats
          .pytest_cache/
        retention-days: 7

    - name: Performance summary
      if: always()
      run: |
        echo "## 🚀 Performance Test Results" >> $GITHUB_STEP_SUMMARY
        echo "Performance tests completed. Check artifacts for detailed reports." >> $GITHUB_STEP_SUMMARY
        if [ -f "reports/profile.stats" ]; then
          echo "- ✅ Performance profile: Generated" >> $GITHUB_STEP_SUMMARY
        fi
        echo "- 📊 Benchmark results: Available in test artifacts" >> $GITHUB_STEP_SUMMARY

  build-and-test-package:
    name: Build and Test Package
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [test]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    - name: Cache Python packages
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.pixi/cache
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml', '**/pixi.lock') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Cache build artifacts
      uses: actions/cache@v4
      with:
        path: |
          dist/
          build/
          *.egg-info/
        key: ${{ runner.os }}-build-${{ github.sha }}
        restore-keys: |
          ${{ runner.os }}-build-

    - name: Build package
      run: pixi run -e dev build

    - name: Check package
      run: pixi run -e dev sh -c "pip install twine && twine check dist/*"

    - name: Test package installation
      run: |
        pixi run -e prod sh -c "pip install dist/*.whl"
        pixi run -e prod python -c "import llm_task_framework; print(\"Package imported successfully\")"
        pixi run -e prod llm-task-framework --help

    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: dist-packages
        path: dist/
        retention-days: 30

  # Performance baseline (non-blocking)
  performance:
    name: Performance Baseline
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [test]
    continue-on-error: true

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    # Pixi handles editable install automatically via environment setup.

    - name: Install benchmark dependencies
      run: pixi run -e dev pip install pytest-benchmark

    - name: Run performance tests (isolated)
      run: |
        echo "Running performance benchmarks in isolation..."
        # Run performance tests separately to avoid interference with other test types
        # Exclude slow tests and run only benchmark-marked tests
        pixi run -e dev pytest tests -m "benchmark and not slow" --benchmark-only --benchmark-json=benchmark.json --no-cov || [ $? -eq 5 ]

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: benchmark.json
        retention-days: 30

    - name: Analyze benchmark results
      run: |
        echo "Analyzing benchmark results..."
        # Use a default baseline if one doesn't exist
        BASELINE_FILE="benchmark_baseline.json"
        if [ ! -f "$BASELINE_FILE" ]; then
          echo "No baseline found, creating an empty baseline file"
          echo '{"benchmarks": []}' > "$BASELINE_FILE"
        fi
        python scripts/analyze_benchmarks.py --baseline benchmark_baseline.json --current benchmark.json --regression-threshold 0.1 > benchmark_report.md

    - name: Post benchmark results to PR
      if: github.event_name == 'pull_request'
      run: |
        echo "Posting benchmark results to PR..."
        python scripts/post_benchmark_comment.py --report benchmark_report.md --repo ${{ github.repository }} --pr ${{ github.event.pull_request.number }} --github-token ${{ secrets.GITHUB_TOKEN }}

    - name: Store benchmark results for future comparison
      run: |
        echo "Storing benchmark results for future comparison..."
        cp benchmark.json benchmark_baseline.json

  # Documentation build check
  docs:
    name: Documentation
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    - name: Cache Python packages
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.pixi/cache
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml', '**/pixi.lock') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Cache documentation build
      uses: actions/cache@v4
      with:
        path: |
          site/
          .mkdocs_cache/
        key: ${{ runner.os }}-docs-${{ hashFiles('docs/**', 'mkdocs.yml', 'README.md') }}
        restore-keys: |
          ${{ runner.os }}-docs-

    - name: Build documentation
      run: pixi run -e docs docs-build

    - name: Upload documentation
      uses: actions/upload-artifact@v4
      with:
        name: documentation
        path: site/
        retention-days: 30

  # Final status check
  ci-success:
    name: CI Success
    runs-on: ubuntu-latest
    needs: [quick-checks, quality-checks, test, integration-tests, build-and-test-package, docs]
    if: always()

    steps:
    - name: Check CI status
      run: |
        if [[ "${{ needs.quick-checks.result }}" != "success" ]]; then
          echo "Quick checks failed"
          exit 1
        fi
        if [[ "${{ needs.quality-checks.result }}" != "success" ]]; then
          echo "Quality checks failed"
          exit 1
        fi
        if [[ "${{ needs.test.result }}" != "success" ]]; then
          echo "Tests failed"
          exit 1
        fi
        # Accept integration-tests result as success, skipped, or cancelled (timeout)
        if [[ "${{ needs.integration-tests.result }}" != "success" && "${{ needs.integration-tests.result }}" != "skipped" && "${{ needs.integration-tests.result }}" != "cancelled" ]]; then
          echo "Integration tests failed"
          exit 1
        fi
        if [[ "${{ needs.build-and-test-package.result }}" != "success" ]]; then
          echo "Package build failed"
          exit 1
        fi
        if [[ "${{ needs.docs.result }}" != "success" ]]; then
          echo "Documentation build failed"
          exit 1
        fi
        echo "All CI checks passed!"
