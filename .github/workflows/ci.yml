name: CI

on:
  push:
    branches: [ main, master, development ]
  pull_request:
    branches: [ main, master, development ]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHONPATH: src
  PYTHONUNBUFFERED: "1"
  FORCE_COLOR: "1"

jobs:
  # Smart change detection for conditional job execution
  detect-changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    timeout-minutes: 2
    outputs:
      has-python-changes: ${{ steps.changes.outputs.python }}
      has-docs-changes: ${{ steps.changes.outputs.docs }}
      has-security-changes: ${{ steps.changes.outputs.security }}
      has-performance-changes: ${{ steps.changes.outputs.performance }}
      has-ci-changes: ${{ steps.changes.outputs.ci }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Detect file changes
      uses: dorny/paths-filter@v2
      id: changes
      with:
        filters: |
          python:
            - 'src/**'
            - 'tests/**'
            - 'pyproject.toml'
            - 'pixi.lock'
          docs:
            - 'docs/**'
            - 'mkdocs.yml'
            - '*.md'
          security:
            - '.secrets.baseline'
            - 'security/**'
            - '.github/workflows/security.yml'
          performance:
            - 'tests/**/test_*performance*'
            - 'tests/**/test_*benchmark*'
            - 'locustfile.py'
          ci:
            - '.github/workflows/**'
            - 'pyproject.toml'
            - 'pixi.lock'

    - name: Change detection summary
      run: |
        echo "## 🔍 Change Detection Results" >> $GITHUB_STEP_SUMMARY
        echo "| Component | Changed | Action |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|---------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Python Code | ${{ steps.changes.outputs.python }} | ${{ steps.changes.outputs.python == 'true' && 'Run full test suite' || 'Skip extended testing' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Documentation | ${{ steps.changes.outputs.docs }} | ${{ steps.changes.outputs.docs == 'true' && 'Build docs' || 'Skip docs build' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Security | ${{ steps.changes.outputs.security }} | ${{ steps.changes.outputs.security == 'true' && 'Run security scans' || 'Basic security only' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Performance | ${{ steps.changes.outputs.performance }} | ${{ steps.changes.outputs.performance == 'true' && 'Run performance tests' || 'Skip performance tests' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| CI/CD | ${{ steps.changes.outputs.ci }} | ${{ steps.changes.outputs.ci == 'true' && 'Full validation' || 'Standard validation' }} |" >> $GITHUB_STEP_SUMMARY

  # Quick feedback loop - fast checks for immediate developer feedback
  quick-checks:
    name: Quick Validation (Fast Feedback)
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    - name: Fast linting check
      run: pixi run -e dev lint || true

    - name: Fast unit tests (no coverage)
      run: pixi run -e dev test-fast -m "not integration and not mcp" || true

    - name: Fast package import test
      run: |
        pixi run -e dev python -c "import llm_task_framework; print('✅ Package imports successfully')" || echo "❌ Package import failed"

    - name: Quick feedback summary
      if: always()
      run: |
        echo "## ⚡ Quick Feedback Results" >> $GITHUB_STEP_SUMMARY
        echo "Fast validation completed in under 5 minutes for immediate developer feedback." >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Note:** Full CI pipeline continues with comprehensive testing, quality checks, and security scanning." >> $GITHUB_STEP_SUMMARY

  quality-checks:
    name: Quality Checks
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [detect-changes, quick-checks]
    if: needs.detect-changes.outputs.has-python-changes == 'true' || needs.detect-changes.outputs.has-ci-changes == 'true'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    - name: Check code formatting
      run: pixi run -e dev format-check

    - name: Run linting
      run: pixi run -e dev lint

    - name: Run type checking
      run: pixi run -e dev typecheck

    - name: Run enhanced security scans
      run: pixi run -e security security-ci

    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports-${{ github.run_number }}
        path: |
          reports/bandit-report.json
          reports/safety-report.json
          reports/pip-audit-report.json
        retention-days: 30

    - name: Security scan summary
      if: always()
      run: |
        echo "## 🛡️ Security Scan Results" >> $GITHUB_STEP_SUMMARY
        echo "Security scans completed. Check artifacts for detailed reports." >> $GITHUB_STEP_SUMMARY
        if [ -f "reports/bandit-report.json" ]; then
          echo "- ✅ Static analysis (Bandit): Report generated" >> $GITHUB_STEP_SUMMARY
        fi
        if [ -f "reports/safety-report.json" ]; then
          echo "- ✅ Dependency scan (Safety): Report generated" >> $GITHUB_STEP_SUMMARY
        fi
        if [ -f "reports/pip-audit-report.json" ]; then
          echo "- ✅ Package audit (pip-audit): Report generated" >> $GITHUB_STEP_SUMMARY
        fi

  test:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    timeout-minutes: 20
    needs: [detect-changes]
    if: needs.detect-changes.outputs.has-python-changes == 'true' || needs.detect-changes.outputs.has-ci-changes == 'true' || github.event_name == 'workflow_dispatch'
    strategy:
      fail-fast: false
      matrix:
        include:
          # Ubuntu: Full Python version coverage (primary development platform)
          - os: ubuntu-latest
            python-version: "3.10"
          - os: ubuntu-latest
            python-version: "3.11"
          - os: ubuntu-latest
            python-version: "3.12"

          # macOS: Python 3.12+ only (aligned with external patterns)
          - os: macos-latest
            python-version: "3.12"

          # Windows: Core versions with performance optimizations
          - os: windows-latest
            python-version: "3.10"
            test-marker: "not slow"  # Skip slow tests on Windows 3.10
          - os: windows-latest
            python-version: "3.12"

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    # Removed explicit editable install step; Pixi handles editable install automatically via environment setup.

    - name: Run unit tests with coverage
      shell: bash
      run: |
        # Run unit tests first to avoid URL patching pollution from integration tests
        # Exclude integration and mcp tests from main test matrix to prevent pollution
        # Use platform-specific optimizations for test execution
        if [[ "${{ matrix.test-marker }}" != "" ]]; then
          echo "Running optimized unit tests for ${{ matrix.os }} Python ${{ matrix.python-version }}..."
          pixi run -e dev pytest tests -m "not integration and not mcp and not benchmark and ${{ matrix.test-marker }}" --cov=llm_task_framework --cov-report=xml:coverage.xml --cov-report=term --maxfail=5 -x
        else
          echo "Running standard unit tests for ${{ matrix.os }} Python ${{ matrix.python-version }}..."
          pixi run -e dev pytest tests -m "not integration and not mcp and not benchmark" --cov=llm_task_framework --cov-report=xml:coverage.xml --cov-report=term
        fi
      env:
        # Disable LLM tests in CI unless API keys provided
        SKIP_LLM_TESTS: ${{ !secrets.ANTHROPIC_API_KEY && !secrets.OPENAI_API_KEY }}

    - name: Run basic integration tests (cross-platform)
      shell: bash
      run: |
        # Run integration tests separately to prevent URL patching pollution
        # Only run basic integration tests that don't require external services
        # Skip on platforms where integration tests might be flaky
        if [[ "${{ matrix.os }}" == "ubuntu-latest" ]] || [[ "${{ matrix.os }}" == "macos-latest" ]]; then
          echo "Running cross-platform integration tests..."
          pixi run -e dev pytest tests -m "integration and not mcp and not benchmark" --cov=llm_task_framework --cov-append || [ $? -eq 5 ]
        else
          echo "Skipping integration tests on ${{ matrix.os }} for performance optimization"
        fi
      env:
        # Disable LLM tests in CI unless API keys provided
        SKIP_LLM_TESTS: ${{ !secrets.ANTHROPIC_API_KEY && !secrets.OPENAI_API_KEY }}

    - name: Upload coverage to Codecov
      if: matrix.python-version == '3.11' && matrix.os == 'ubuntu-latest'
      uses: codecov/codecov-action@v3
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  integration-tests:
    name: Advanced Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [test]
    if: github.event_name != 'pull_request' || github.event.pull_request.draft == false

    # Service containers for enhanced integration testing (Ubuntu only)
    services:
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

      websocket-echo:
        image: jmalloc/echo-server:latest
        options: >-
          --health-cmd "wget --spider -q http://localhost:8080/ || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 8080:8080

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    # Pixi handles editable install automatically via environment setup.

    - name: Wait for Redis service
      run: |
        echo "Waiting for Redis service to be ready..."
        # Install redis-tools for health checks
        sudo apt-get update && sudo apt-get install -y redis-tools
        # Wait for Redis with timeout, fail if not ready
        timeout 30 bash -c 'until redis-cli -h localhost -p 6379 ping | grep PONG; do sleep 1; done'
        echo "✅ Redis service is ready."

    - name: Wait for WebSocket echo service
      run: |
        echo "Waiting for WebSocket echo service to be ready..."
        # Wait for WebSocket echo server with timeout, fail if not ready
        timeout 30 bash -c 'until wget --spider -q http://localhost:8080/; do sleep 1; done'
        echo "✅ WebSocket echo service is ready."

    - name: Run advanced integration tests
      run: |
        echo "Running advanced integration tests with external services..."
        # Run advanced integration tests that may require external services
        # These run separately to avoid pollution of cross-platform tests
        pixi run -e dev pytest tests -m "integration and not benchmark" --cov=llm_task_framework --cov-report=xml:integration-coverage.xml --cov-fail-under=0 || [ $? -eq 5 ]
      env:
        # LLM API keys
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        # Service container connection details
        REDIS_URL: redis://localhost:6379
        WEBSOCKET_URL: ws://localhost:8080
        TEST_WITH_SERVICES: true

    - name: Run MCP server tests (isolated)
      run: |
        echo "Running MCP server tests in isolation..."
        # Run MCP tests separately to prevent any server state pollution
        pixi run -e dev pytest tests -m "mcp and not benchmark" --cov=llm_task_framework --cov-append --cov-fail-under=0 || [ $? -eq 5 ]
      env:
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

    - name: Upload integration coverage to Codecov
      uses: codecov/codecov-action@v3
      if: always()
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        file: ./integration-coverage.xml
        flags: integration
        name: codecov-integration
        fail_ci_if_error: false

  performance-tests:
    name: Performance Testing
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [detect-changes, test]
    if: needs.detect-changes.outputs.has-performance-changes == 'true' || needs.detect-changes.outputs.has-python-changes == 'true' || github.event_name == 'workflow_dispatch'
    continue-on-error: true  # Don't fail CI if performance tests have issues

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    - name: Run benchmark tests
      run: |
        echo "Running performance benchmark tests..."
        pixi run -e performance perf-test || echo "Performance tests completed with warnings"

    - name: Generate performance profile
      run: |
        echo "Generating performance profile..."
        pixi run -e performance perf-profile || echo "Performance profiling completed"

    - name: Upload performance reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-reports-${{ github.run_number }}
        path: |
          reports/profile.stats
          .pytest_cache/
        retention-days: 7

    - name: Performance summary
      if: always()
      run: |
        echo "## 🚀 Performance Test Results" >> $GITHUB_STEP_SUMMARY
        echo "Performance tests completed. Check artifacts for detailed reports." >> $GITHUB_STEP_SUMMARY
        if [ -f "reports/profile.stats" ]; then
          echo "- ✅ Performance profile: Generated" >> $GITHUB_STEP_SUMMARY
        fi
        echo "- 📊 Benchmark results: Available in test artifacts" >> $GITHUB_STEP_SUMMARY

  build-and-test-package:
    name: Build and Test Package
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [test]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    - name: Build package
      run: pixi run -e dev build

    - name: Check package
      run: pixi run -e dev sh -c "pip install twine && twine check dist/*"

    - name: Test package installation
      run: |
        pixi run -e prod sh -c "pip install dist/*.whl"
        pixi run -e prod python -c "import llm_task_framework; print(\"Package imported successfully\")"
        pixi run -e prod llm-task-framework --help

    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: dist-packages
        path: dist/
        retention-days: 30

  # Performance baseline (non-blocking)
  performance:
    name: Performance Baseline
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [test]
    continue-on-error: true

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    # Pixi handles editable install automatically via environment setup.

    - name: Install benchmark dependencies
      run: pixi run -e dev pip install pytest-benchmark

    - name: Run performance tests (isolated)
      run: |
        echo "Running performance benchmarks in isolation..."
        # Run performance tests separately to avoid interference with other test types
        # Exclude slow tests and run only benchmark-marked tests
        pixi run -e dev pytest tests -m "benchmark and not slow" --benchmark-only --benchmark-json=benchmark.json --no-cov || [ $? -eq 5 ]

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: benchmark.json
        retention-days: 30

  # Documentation build check
  docs:
    name: Documentation
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Pixi
      uses: prefix-dev/setup-pixi@v0.8.8
      with:
        pixi-version: v0.41.4
        cache: true

    - name: Build documentation
      run: pixi run -e docs docs-build

    - name: Upload documentation
      uses: actions/upload-artifact@v4
      with:
        name: documentation
        path: site/
        retention-days: 30

  # Final status check
  ci-success:
    name: CI Success
    runs-on: ubuntu-latest
    needs: [quick-checks, quality-checks, test, integration-tests, build-and-test-package, docs]
    if: always()

    steps:
    - name: Check CI status
      run: |
        if [[ "${{ needs.quick-checks.result }}" != "success" ]]; then
          echo "Quick checks failed"
          exit 1
        fi
        if [[ "${{ needs.quality-checks.result }}" != "success" ]]; then
          echo "Quality checks failed"
          exit 1
        fi
        if [[ "${{ needs.test.result }}" != "success" ]]; then
          echo "Tests failed"
          exit 1
        fi
        if [[ "${{ needs.integration-tests.result }}" != "success" && "${{ needs.integration-tests.result }}" != "skipped" ]]; then
          echo "Integration tests failed"
          exit 1
        fi
        if [[ "${{ needs.build-and-test-package.result }}" != "success" ]]; then
          echo "Package build failed"
          exit 1
        fi
        if [[ "${{ needs.docs.result }}" != "success" ]]; then
          echo "Documentation build failed"
          exit 1
        fi
        echo "All CI checks passed!"
